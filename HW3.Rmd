---
title: "P8105_HW3_cc4778"
author: "Chee Kay Cheong"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(patchwork)
library(p8105.datasets)
```

# Problem 1

```{r distinct users, echo = FALSE}
data("instacart")

n_users =  
  instacart %>% 
  select(user_id) %>% 
  distinct()
```

The `instacart` dataset contains `r ncol(instacart)` variables and `r nrow(instacart)` observations, where each row in the dataset is a product from an order. 
There is a single order per user, and there are `r nrow(n_users)` unique users in this dataset.

### Questions

1. How many aisles are there, and which aisles are the most items ordered from?
```{r Q1}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```
There are 134 distinct aisles, and most items are ordered from the "Fresh vegetables" aisle.


2. Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. 
Arrange aisles sensibly, and organize your plot so others can read it.
```{r Q2, fig.width = 16, fig.height = 12}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n, fill = aisle)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  geom_text(aes(label = n),
            position = position_stack(vjust = 1)) +
  labs(
    title = "Number of items ordered in each aisle",
    y = "Number of items ordered") +
  theme(legend.key.size = unit(1.2, 'cm'))
```


3. Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. 
Include the number of times each item is ordered in your table.
```{r Q3}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```


4. Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.
Format this table for human readers (i.e. produce a 2 x 7 table)
```{r Q4}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hour) %>%  
  # kind of act like pivot_wider
  knitr::kable(digits = 2)
```


# Problem 2

```{r read data, message = FALSE}
accel = 
  read_csv("Data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count") %>% 
  mutate(
    week_ = ifelse(day %in% c("Saturday", "Sunday"), "Weekend", "Weekday"),
    minute = as.numeric(minute))
```
After tidying the `accel` dataset, it now has `r ncol(accel)` variables and `r nrow(accel)` observations.

The variables are:

* `week` : numeric 

* `day_id` : numeric

* `day` : character

* `minute` : numeric

* `activity_count` : numeric

* `week_` : character


```{r total activity variable, message = FALSE}
accel %>% 
  mutate(day = fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>% 
  group_by(week, day) %>% 
  summarize(
    total_activity_per_day = sum(activity_count)) %>% 
  pivot_wider(
    names_from = "day",
    values_from = "total_activity_per_day") %>% 
 knitr::kable(digits = 1)
```
There are 5 weeks and 35 days, but the `day_id` does not seem to be consistent with the sequence of day in a week, 
so I rearranged the days in each week to a normal, reasonable sequence. 
It seems like this individual would typically have more than 100,000 activities counted each day, 
except for the first Monday and two Saturdays (one on the 24th day, another one on the 31st day), 
in which the activity count on both days are 1440. 


```{r activity vs time graph, fig.width = 16, fig.height = 12, message = FALSE}
accel %>% 
  mutate(day = fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>% 
  group_by(day) %>% 
  ggplot(aes(x = minute, y = activity_count, color = day)) +
  geom_smooth(se = FALSE) + 
  facet_grid(. ~ day) + 
  labs(
    title = "Activity count per minute for 24 hour",
    x = "Minute (24 hour)",
    y = "Activity count",
    caption = "Accelerometer data")
```
Based on the graph, in most days of a week, the highest number of activity count occurred after 1000 minute, which is around 5pm. 
However, on Sunday, the highest number of activity count occurred at about 600 minute, which is around 10am. 
In conclusion, this individual typically has highest number of activity count during the late afternoon to early evening every day,
except for Sunday.


# Problem 3

```{r load ny noaa data}
data("ny_noaa")

noaa =
  ny_noaa %>%  
  janitor::clean_names() %>% 
  mutate(
    tmax = as.numeric(tmax),
    tmin = as.numeric(tmin)) %>% 
  separate(date, into = c("year", "month", "day")) %>% 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day)) %>% 
  mutate(
    prcp = prcp / 10,
    tmax = tmax / 10,
    tmin = tmin / 10)
```
After loading and cleaning the `noaa` dataset, it now contains `r ncol(noaa)` variables and `r nrow(noaa)` observations. All observations for temperatures, precipitation, and snowfall are also in reasonable units (mm). The data size is massive, and there is a lot of missing variables across different columns. We cannot simply remove those missing values because we are not sure they can be safely removed without interfering with the analysis. Also, these missing values might impact our mathematical computation, and we need to exclude them during computation.

```{r snowfall common value}
noaa %>% 
  group_by(snow) %>% 
  summarize(
    n_times_appear = n()) %>% 
    arrange(desc(n_times_appear))
```
For snowfall, the most commonly observed value is **0**, meaning 0 mm of snowfall. I believe that is because snowfall only occurs
during winter, and it doesn't occur every day during the entire winter season. This dataset contains daily snowfall information
which spans across decades from many different weather stations, so it would be normal to see 0 mm of snowfall most commonly across
this dataset.


```{r mean tmax in Jan & July across staions, message = FALSE}
noaa_tmax_jan =
  noaa %>% 
  filter(month == 1) %>% 
  group_by(id, year) %>% 
  summarize(
    average_tmax = round(mean(tmax, na.rm = TRUE), 2)) %>% 
  filter(!is.na(average_tmax))
  
noaa_tmax_july = 
  noaa %>% 
  filter(month == 7) %>% 
  group_by(id, year) %>% 
  summarize(
    average_tmax = round(mean(tmax, na.rm = TRUE), 2)) %>% 
  filter(!is.na(average_tmax))
```

```{r two panel plot, fig.width = 15, fig.height = 12, message = FALSE}
tmax_jan_plot =
  noaa_tmax_jan %>% 
  ggplot(aes(x = year, y = average_tmax, group = id)) +
  geom_point(alpha = 0.3) + 
  labs(
    x = "Years",
    y = "Average maximum temperature (C)",
    color = "Station",
    title = "Average maximum temperature in January",
    caption = "Data come from the ny_noaa") +
  scale_x_continuous(breaks = seq(1980, 2010, 1)) +
  theme_gray() +
  theme(legend.position = "none")

tmax_july_plot = 
  noaa_tmax_july %>% 
  ggplot(aes(x = year, y = average_tmax, group = id)) +
  geom_point(alpha = 0.3) +
  labs(
    x = "Years",
    y = "Average maximum temperature (C)",
    title = "Average maximum temperature in July",
    caption = "Data come from the ny_noaa") +
  scale_x_continuous(breaks = seq(1980, 2010, 1)) +
  theme_gray()

tmax_jan_plot / tmax_july_plot
```
Based on the above two graphs, we can see that the average maximum temperatures in both January and July from each station followed a normal distribution, with some outliers, 
in each year. In addition, if we compared the average maximum temperatures of January and July for the year 1981 and 2010, we can see that there is an increase in average maximum
temperature, suggesting the evidence of global warming. 



```{r temp & snow, fig.width = 15, fig.height = 12, message = FALSE}
temp_plot =
  noaa %>% 
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex(na.rm = TRUE) +
  labs(
    x = "Minimum temperature (C)",
    y = "Maximum temperature (C)",
    title = "Maximum temperature vs. Minimum temperature",
    caption = "Data comes from 'ny_noaa'")


snow_plot =
  noaa %>% 
  filter((snow > 0) & (snow < 100)) %>%
  ggplot(aes(x = year, y = snow, group = year)) + 
  geom_boxplot() +
  labs(
    x = "Year",
    y = "Snowfall (mm)",
    title = "Snowfall Distribution of each year",
    caption = "Data comes from 'ny_noaa'") +
  scale_x_continuous(breaks = seq(1980, 2010, 1)) +
  theme_gray()

temp_plot / snow_plot
```

