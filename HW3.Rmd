---
title: "P8105_HW3_cc4778"
author: "Chee Kay Cheong"
output: github_document
---

# Problem 1

```{r setup, include = FALSE}
library(tidyverse)
library(patchwork)
library(p8105.datasets)

data("instacart")
```

```{r distinct users, echo = FALSE}
n_users =  
  instacart %>% 
  select(user_id) %>% 
  distinct()
```

The `instacart` dataset contains `r ncol(instacart)` variables and `r nrow(instacart)` observations, where each row in the dataset is a product from an order. There is a single order per user, and there are `r nrow(n_users)` unique users in this dataset.

### Questions

1. How many aisles are there, and which aisles are the most items ordered from?
```{r Q1}
instacart %>% 
  select(aisle_id, aisle) %>% 
  distinct() 
# There are 134 distinct aisles.
instacart %>% 
  group_by(aisle) %>% 
  summarize(
    n_order = n()) %>% 
  arrange(desc(n_order))
# Most items are ordered from the "Fresh vegetables" aisle.
```

2. Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.
```{r Q2, fig.width = 16, fig.height = 12}
instacart %>% 
  group_by(aisle) %>% 
  summarize(
    n_order = n()) %>% 
  filter(n_order > 10000) %>% 
  ggplot(aes(x = aisle, y = n_order, fill = aisle)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  geom_text(aes(label = n_order),
            position = position_stack(vjust = 1)) +
  labs(
    title = "Number of items ordered in each aisle",
    y = "Number of items ordered") +
  theme(legend.key.size = unit(1.2, 'cm'))
```

3. Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
```{r Q3}

```


# Problem 2

```{r read data, message = FALSE}
accel = 
  read_csv("Data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count") %>% 
  mutate(
    week_ = ifelse(day %in% c("Saturday", "Sunday"), "Weekend", "Weekday"),
    minute = as.numeric(minute))
```
After tidying the `accel` dataset, it now has `r ncol(accel)` variables and `r nrow(accel)` observations.

The variables are:

* `week` : numeric 

* `day_id` : numeric

* `day` : character

* `minute` : numeric

* `activity_count` : numeric

* `week_` : character

```{r total activity variable, message = FALSE}
accel %>% 
  group_by(week, day_id, day) %>% 
  summarize(
    total_activity_per_day = sum(activity_count)) %>% 
 knitr::kable(digits = 1)
```
There are 5 weeks and 35 days, but the `day_id` does not seem to be consistent with the sequence of day in a week. 
It seems like this individual would typically have more than 100000 activities counted every day, except for two Saturdays 
(one on the 24th day, another one on the 31st day), in which the activity count on both days are 1440. 

```{r activity vs time graph, fig.width = 16, fig.height = 12, message = FALSE}
accel %>% 
  mutate(day = fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>% 
  group_by(day) %>% 
  ggplot(aes(x = minute, y = activity_count, color = day)) +
  geom_smooth(se = FALSE) + 
  facet_grid(. ~ day) + 
  labs(
    title = "Activity count per minute for 24 hour",
    x = "Minute (24 hour)",
    y = "Activity count",
    caption = "Accelerometer data")
```



# Problem 3

```{r load ny noaa data}
data("ny_noaa")

ny_noaa =
  ny_noaa %>%  
  janitor::clean_names() %>% 
  mutate(
    tmax = as.integer(tmax),
    tmin = as.integer(tmin)) %>% 
  separate(date, into = c("year", "month", "day")) %>% 
  mutate(ny_noaa,
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day),
    prcp = prcp / 10,
    tmax = tmax / 10,
    tmin = tmin / 10)
```
After loading and cleaning the `ny_noaa` dataset, it now contains `r ncol(ny_noaa` variables and `r nrow(ny_noaa)` observations. All observations for temperatures, precipitation, and snowfall are also in reasonable units (mm). The data size is massive, and there is a lot of missing variables across different columns. We cannot simply remove those missing values because we are not sure they can be safely removed without interfering with the analysis. Also, these missing values might impact our mathematical computation :( 

```{r snowfall common value}
ny_noaa %>% 
  group_by(snow) %>% 
  mutate(
    value = mean(snow, na.rm = TRUE)
  )
  
  
```






