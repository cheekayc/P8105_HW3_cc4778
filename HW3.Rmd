---
title: "P8105_HW3_cc4778"
author: "Chee Kay Cheong"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(patchwork)
library(p8105.datasets)

data("instacart")
```

# Problem 1

```{r distinct users, echo = FALSE}
n_users =  
  instacart %>% 
  select(user_id) %>% 
  distinct()
```

The `instacart` dataset contains `r ncol(instacart)` variables and `r nrow(instacart)` observations, where each row in the dataset is a product from an order. There is a single order per user, and there are `r nrow(n_users)` unique users in this dataset.

### Questions

1. How many aisles are there, and which aisles are the most items ordered from?
```{r Q1}
instacart %>% 
  select(aisle_id, aisle) %>% 
  distinct() 
# There are 134 distinct aisles.
instacart %>% 
  group_by(aisle) %>% 
  summarize(
    n_order = n()) %>% 
  arrange(desc(n_order))
# Most items are ordered from the "Fresh vegetables" aisle.
```

2. Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.
```{r Q2, fig.width = 16, fig.height = 12}
instacart %>% 
  group_by(aisle) %>% 
  summarize(
    n_order = n()) %>% 
  filter(n_order > 10000) %>% 
  ggplot(aes(x = aisle, y = n_order, fill = aisle)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  geom_text(aes(label = n_order),
            position = position_stack(vjust = 1)) +
  labs(
    title = "Number of items ordered in each aisle",
    y = "Number of items ordered") +
  theme(legend.key.size = unit(1.2, 'cm'))
```

3. Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
```{r Q3}

```


# Problem 2

```{r read data, message = FALSE}
accel = 
  read_csv("Data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count") %>% 
  mutate(
    week_ = ifelse(day %in% c("Saturday", "Sunday"), "Weekend", "Weekday"),
    minute = as.numeric(minute))
```
After tidying the `accel` dataset, it now has `r ncol(accel)` variables and `r nrow(accel)` observations.

The variables are:

* `week` : numeric 

* `day_id` : numeric

* `day` : character

* `minute` : numeric

* `activity_count` : numeric

* `week_` : character


```{r total activity variable, message = FALSE}
accel %>% 
  mutate(day = fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>% 
  group_by(week, day) %>% 
  summarize(
    total_activity_per_day = sum(activity_count)) %>% 
 knitr::kable(digits = 1)
```
There are 5 weeks and 35 days, but the `day_id` does not seem to be consistent with the sequence of day in a week, 
so I rearranged the days in each week to a normal, reasonable sequence. 
It seems like this individual would typically have more than 100,000 activities counted each day, 
except for the first Monday and two Saturdays (one on the 24th day, another one on the 31st day), 
in which the activity count on both days are 1440. 


```{r activity vs time graph, fig.width = 16, fig.height = 12, message = FALSE}
accel %>% 
  mutate(day = fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>% 
  group_by(day) %>% 
  ggplot(aes(x = minute, y = activity_count, color = day)) +
  geom_smooth(se = FALSE) + 
  facet_grid(. ~ day) + 
  labs(
    title = "Activity count per minute for 24 hour",
    x = "Minute (24 hour)",
    y = "Activity count",
    caption = "Accelerometer data")
```
Based on the graph, in most days of a week, the highest number of activity count occurred after 1000 minute, which is around 5pm. 
However, on Sunday, the highest number of activity count occurred at about 600 minute, which is around 10am. 
In conclusion, this individual typically has highest number of activity count during the late afternoon to early evening every day,
except for Sunday.


# Problem 3

```{r load ny noaa data}
data("ny_noaa")

noaa =
  ny_noaa %>%  
  janitor::clean_names() %>% 
  mutate(
    tmax = as.numeric(tmax),
    tmin = as.numeric(tmin)) %>% 
  separate(date, into = c("year", "month", "day")) %>% 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day)) %>% 
  mutate(
    prcp = prcp / 10,
    tmax = tmax / 10,
    tmin = tmin / 10)
```
After loading and cleaning the `noaa` dataset, it now contains `r ncol(noaa)` variables and `r nrow(noaa)` observations. All observations for temperatures, precipitation, and snowfall are also in reasonable units (mm). The data size is massive, and there is a lot of missing variables across different columns. We cannot simply remove those missing values because we are not sure they can be safely removed without interfering with the analysis. Also, these missing values might impact our mathematical computation, and we need to exclude them during computation.

```{r snowfall common value}
noaa %>% 
  group_by(snow) %>% 
  summarize(
    n_times_appear = n()) %>% 
    arrange(desc(n_times_appear))
```
For snowfall, the most commonly observed value is **0**, meaning 0 mm of snowfall. I believe that is because snowfall only occurs
during winter, and it doesn't occur every day during the entire winter season. This dataset contains daily snowfall information
which spans across decades from many different weather stations, so it would be normal to see 0 mm of snowfall most commonly across
this dataset.







